{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural machine translate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9MYDYsD2bId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5jeo670334w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip \"/content/drive/My Drive/fra-eng.zip\" -d \"/content/drive/My Drive/fra-eng/\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzhKPHl3FWYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSMIe1YvGFAA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "43371335-0648-4a4e-e8a8-2fc0b7bede7a"
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"多Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_HwwBZrH-z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYuD_Z0CITc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "98c5612c-c03c-4481-8949-934d33264376"
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
        "\n",
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n",
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQr6jlidI5av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kUuhkUrM1fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  inp_lang, targ_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKEiDq4HM4Ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfJM-t5EM9l6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dcad227c-db6a-4b08-b8be-9887b9f2bca1"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24000 24000 6000 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ER1_79RNS4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eie3UEBDNUNv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "f6d9eb58-007e-4ed1-c13b-bb26384757f4"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "20 ----> that\n",
            "11 ----> s\n",
            "85 ----> so\n",
            "568 ----> perfect\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "7 ----> es\n",
            "596 ----> perfecto\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KxTQruWNYgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_ZAkMe8NahY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f0e3f04-64b6-4712-9075-24b82e0816f8"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 11]), TensorShape([64, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyQ35pw-Kelb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZIuV8GhXQrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAYEYHEOYq2e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f34b5b1-9397-4f00-d18b-01f26c8ba1d2"
      },
      "source": [
        "print(sample_output.shape, sample_hidden.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 11, 1024) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXi7fs1rYwlu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "c4c013ac-dcc9-4655-fe96-dd61098dba26"
      },
      "source": [
        "sample_output[:, -1]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
              "array([[ 2.3334401e-03, -3.3441800e-04,  1.2556351e-05, ...,\n",
              "         9.2404168e-03,  7.8039146e-03,  7.6951645e-03],\n",
              "       [ 2.1323622e-03, -5.3371932e-05,  2.1659576e-04, ...,\n",
              "         9.2736725e-03,  7.4133738e-03,  7.9543088e-03],\n",
              "       [ 2.1700012e-03, -1.8573692e-04,  5.7292840e-04, ...,\n",
              "         8.7366235e-03,  7.1453899e-03,  7.6010148e-03],\n",
              "       ...,\n",
              "       [ 2.8647305e-03, -4.2563384e-05, -1.3540374e-04, ...,\n",
              "         9.5746294e-03,  6.5460727e-03,  7.6617352e-03],\n",
              "       [ 3.8258780e-03, -7.8505598e-04,  3.9330797e-04, ...,\n",
              "         8.8794138e-03,  5.0382488e-03,  6.9850576e-03],\n",
              "       [ 2.3208731e-03,  2.7603275e-04,  4.6823491e-04, ...,\n",
              "         9.1635613e-03,  7.6440782e-03,  7.9254946e-03]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEQhJp3KZMSv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "a552652d-ec75-444a-83c0-0e93c61289c8"
      },
      "source": [
        "sample_hidden"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
              "array([[ 2.3334401e-03, -3.3441800e-04,  1.2556351e-05, ...,\n",
              "         9.2404168e-03,  7.8039146e-03,  7.6951645e-03],\n",
              "       [ 2.1323622e-03, -5.3371932e-05,  2.1659576e-04, ...,\n",
              "         9.2736725e-03,  7.4133738e-03,  7.9543088e-03],\n",
              "       [ 2.1700012e-03, -1.8573692e-04,  5.7292840e-04, ...,\n",
              "         8.7366235e-03,  7.1453899e-03,  7.6010148e-03],\n",
              "       ...,\n",
              "       [ 2.8647305e-03, -4.2563384e-05, -1.3540374e-04, ...,\n",
              "         9.5746294e-03,  6.5460727e-03,  7.6617352e-03],\n",
              "       [ 3.8258780e-03, -7.8505598e-04,  3.9330797e-04, ...,\n",
              "         8.8794138e-03,  5.0382488e-03,  6.9850576e-03],\n",
              "       [ 2.3208731e-03,  2.7603275e-04,  4.6823491e-04, ...,\n",
              "         9.1635613e-03,  7.6440782e-03,  7.9254946e-03]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiQrvfUOZQ_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "\n",
        "    query_with_time_axis = tf.expand_dims(query, axis = 1)\n",
        "\n",
        "    score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "476_5-lQsuGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ff64f2f6-6bca-4838-bcc3-fb41e2350a51"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 11, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-aqauBEwXHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(dec_units, return_sequences=True, return_state=True)\n",
        "\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNkVoXDP4y6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a547862f-c826-4cd9-b2e7-d691241a517a"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 9414)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFReiUbi_M-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none'\n",
        ")\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwzs9nR5GrPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = 'drive/My Drive/nmt_checkpoint'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ46iFP-HCEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teaching Force\n",
        "\n",
        "    for t in range(1, targ.shape[1]):\n",
        "\n",
        "      predictions, dec_hidden, _  = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "    \n",
        "    batch_loss = loss / int(targ.shape[1])\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYRlxqJDOrm0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7cf05f4b-fadd-4847-f1a8-53196d7117b5"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.1453\n",
            "Epoch 1 Batch 100 Loss 1.6467\n",
            "Epoch 1 Batch 200 Loss 1.5518\n",
            "Epoch 1 Batch 300 Loss 1.2929\n",
            "Epoch 1 Loss 1.5584\n",
            "Time taken for 1 epoch 61.938677072525024 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.2892\n",
            "Epoch 2 Batch 100 Loss 1.1584\n",
            "Epoch 2 Batch 200 Loss 1.0860\n",
            "Epoch 2 Batch 300 Loss 1.0695\n",
            "Epoch 2 Loss 1.0819\n",
            "Time taken for 1 epoch 46.756497383117676 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.7979\n",
            "Epoch 3 Batch 100 Loss 0.8160\n",
            "Epoch 3 Batch 200 Loss 0.8017\n",
            "Epoch 3 Batch 300 Loss 0.6564\n",
            "Epoch 3 Loss 0.7905\n",
            "Time taken for 1 epoch 46.012271881103516 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.5157\n",
            "Epoch 4 Batch 100 Loss 0.5623\n",
            "Epoch 4 Batch 200 Loss 0.5857\n",
            "Epoch 4 Batch 300 Loss 0.5649\n",
            "Epoch 4 Loss 0.5697\n",
            "Time taken for 1 epoch 46.914252042770386 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.4600\n",
            "Epoch 5 Batch 100 Loss 0.4602\n",
            "Epoch 5 Batch 200 Loss 0.4222\n",
            "Epoch 5 Batch 300 Loss 0.4463\n",
            "Epoch 5 Loss 0.4149\n",
            "Time taken for 1 epoch 46.07928419113159 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.2462\n",
            "Epoch 6 Batch 100 Loss 0.3124\n",
            "Epoch 6 Batch 200 Loss 0.2964\n",
            "Epoch 6 Batch 300 Loss 0.3408\n",
            "Epoch 6 Loss 0.3085\n",
            "Time taken for 1 epoch 47.07564091682434 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2337\n",
            "Epoch 7 Batch 100 Loss 0.2536\n",
            "Epoch 7 Batch 200 Loss 0.2371\n",
            "Epoch 7 Batch 300 Loss 0.2475\n",
            "Epoch 7 Loss 0.2350\n",
            "Time taken for 1 epoch 46.21353244781494 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1628\n",
            "Epoch 8 Batch 100 Loss 0.1805\n",
            "Epoch 8 Batch 200 Loss 0.1761\n",
            "Epoch 8 Batch 300 Loss 0.2009\n",
            "Epoch 8 Loss 0.1853\n",
            "Time taken for 1 epoch 47.075679779052734 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1122\n",
            "Epoch 9 Batch 100 Loss 0.1357\n",
            "Epoch 9 Batch 200 Loss 0.1463\n",
            "Epoch 9 Batch 300 Loss 0.1674\n",
            "Epoch 9 Loss 0.1534\n",
            "Time taken for 1 epoch 46.24949288368225 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1197\n",
            "Epoch 10 Batch 100 Loss 0.1357\n",
            "Epoch 10 Batch 200 Loss 0.1821\n",
            "Epoch 10 Batch 300 Loss 0.1176\n",
            "Epoch 10 Loss 0.1331\n",
            "Time taken for 1 epoch 47.077232122421265 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WquGu2YWO3_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(\" \")]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                        dec_hidden,\n",
        "                                                        enc_out)\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34wKoPi80qy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSJFNdqz02iC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnZZQleM1pbW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b88c0392-d0ec-4779-c366-de397a4cb7cf"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe191db49e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DuNyfc-09Tg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "outputId": "7b3456e2-fb00-4411-9ad2-9a9c1111ef2d"
      },
      "source": [
        "translate(u'are you still at home?')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> are you still at home ? <end>\n",
            "Predicted translation: 多 todavia estas en casa ? <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}